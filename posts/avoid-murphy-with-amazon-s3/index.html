<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><title>Avoid murphy with Amazon S3 - Hamilton Lima</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=https://hamiltonlima.comfavicon.png><link rel=canonical href=https://hamiltonlima.com/posts/avoid-murphy-with-amazon-s3/><link rel=stylesheet href=/css/style.min.381fd69187be1f5dde5031ac9014016e77965dbc38358cb065b6fede27b5861c.css><link rel=stylesheet href=/assets/css/extended.min.9729f28a5087b689b946e103d96abd63bfd37b1417effa251dbf798312e3fba5.css><meta property="og:title" content="Avoid murphy with Amazon S3"><meta property="og:type" content="website"><meta property="og:url" content="https://hamiltonlima.com/posts/avoid-murphy-with-amazon-s3/"><meta name=twitter:card content="summary"><meta name=twitter:site content="@athanazio"><meta name=twitter:creator content="@athanazio"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;700&display=swap" rel=stylesheet></head><body class="page frame page-blog-single"><div id=menu-main-mobile class=menu-main-mobile><ul class=menu><li class=menu-item-posts><a href=https://hamiltonlima.com/posts>Posts</a></li></ul></div><div id=wrapper class=wrapper><div class=header><a class=header-logo href=/>Hamilton Lima</a><div class=menu-main><ul><li class=menu-item-posts><a href=/posts><span>Posts</span></a></li></ul></div><div id=toggle-menu-main-mobile class=hamburger-trigger><button class=hamburger>Menu</button></div></div><div class=blog><div class=intro><h1>Avoid murphy with Amazon S3<span class=dot>.</span></h1></div><div class=content><p><img src=/images/2016/11/photo-1462045504115-6c1d931f07d1.jpg alt=photo-1462045504115-6c1d931f07d1> Backups are like life insurance, you allways will be sorry if you don´t have when you need it. That&rsquo;s why I am sharing this step by step way to backup your application or site or wherever you have in production.</p><h3 id=preparing-the-vault-aka-amazon-s3>Preparing the vault, a.k.a. Amazon S3</h3><p>I decided to store my backups on Amazon S3 service, because is <a href=https://aws.amazon.com/s3/pricing/>cheap</a>, around USD0,03 per GB, and very simple to store and to retrieve the data. My first step to try to mount an remote amazon s3 storage as a linux folder at the server, lost some hours trying to make it work, but luckly found enlighment from one comment at stackoverflow: DON&rsquo;T USE IT :) seriously run from it! After been healed from the mount sickness, I just downloaded the <a href=http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3>Amazon Webservice command line tools</a> and everything run smoothly! But as I allways say: Nothing is simple, but everything is possible. The process has several small steps to follow, most of then you only do it once, So this are the steps you need to do at amazon to prepare your backup storage:</p><ul><li>Create you amazon S3 account - <a href=https://aws.amazon.com/free>https://aws.amazon.com/free</a></li><li><a href="https://console.aws.amazon.com/s3/home?region=us-east-1">Create a bucket</a> to save your data</li><li><a href="https://console.aws.amazon.com/iam/home?region=us-east-1#users">Create a user</a> to be access your bucket</li><li>Save the new user access key and secret, you will need it to setup your server</li><li>Give the new user the proper rights to your bucket</li></ul><p>Most of the steps are just click click and click. Give the proper rights to your new user require more setup. The idea is to <a href="https://console.aws.amazon.com/iam/home?region=us-east-1#policies">create a policy</a> and attach an user to it, and the policy has a well defined JSON format, yes a JSON format, <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html>with lots and lots of options</a>. You can do cool stuff like filter by IP, give rights during some period of time and several other cool features. I used the most simple rights: right to read the bucket and the right to create objects in the bucket, this is the policy description I used:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>{
    &#34;Version&#34;: &#34;2012-10-17&#34;,
    &#34;Statement&#34;: \[
        {
            &#34;Action&#34;: \[
                &#34;s3:ListBucket&#34;,
                &#34;s3:GetBucketLocation&#34;
            \],
            &#34;Effect&#34;: &#34;Allow&#34;,
            &#34;Resource&#34;: &#34;arn:aws:s3:::bucketname&#34;
        },
        {
            &#34;Action&#34;: &#34;s3:\*&#34;,
            &#34;Effect&#34;: &#34;Allow&#34;,
            &#34;Resource&#34;: &#34;arn:aws:s3:::bucketname/\*&#34;
        }
    \]
}
</code></pre></div><p>I took me some time to realize that we need two statements in the policy description: Allow ListBucket and GetBucketLocation to the bucket and other to allow all actions to the bucket files, attention to the diference: bucketname vs bucketname/*</p><h3 id=installing-awscli>Installing AWS CLI</h3><p>The installation process is just install, add the access key and secret and test your setup, here are the steps:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>apt install awscli
aws configure
cat ~/.aws/credentials
</code></pre></div><p>After that you can use the aws command line interface, here are some samples:</p><p>aws s3 ls s3://bucketname</p><p>list the existing files in the bucket</p><p>aws s3 cp myfolder s3://bucketname/myfolder &ndash;recursive</p><p>copy myfolder recursively to s3</p><p>see all commands here : <a href=http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3>http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3</a></p><h3 id=the-backup-script>The backup script</h3><p>Now that we have aws installed and the vault ready, let&rsquo;s talk about the backup script, first what if should do:</p><ul><li>save database dump with the timestamp on the file name</li><li>copy all the files from the nodejs application</li><li>copy all the files from wordpress</li><li>copy all the files from the static content</li><li>upload the content to Amazon S3</li></ul><p>So the following script is doing all of it:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=cp>#!/bin/sh
</span><span class=cp></span><span class=c1># get the current timestamp</span>
<span class=nv>NOW</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>date +<span class=s2>&#34;%Y%m%d\_%H%M%S\_%N&#34;</span><span class=k>)</span><span class=s2>&#34;</span>
<span class=nv>TODAY</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>date +<span class=s2>&#34;%Y%m%d&#34;</span><span class=k>)</span><span class=s2>&#34;</span>

<span class=c1># create the backup folder</span>
mkdir /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>
mkdir /tmp/<span class=nv>$TODAY</span>

<span class=c1># dump database data</span>
mysqldump --complete-insert --no-set-names --host<span class=o>=</span>127.0.0.1 --user<span class=o>=</span>backup<span class=se>\_</span>uzr --password<span class=o>=</span>XXXX wordpress &gt; /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>/wordpress.sql
mysqldump --complete-insert --no-set-names --host<span class=o>=</span>127.0.0.1 --user<span class=o>=</span>backup<span class=se>\_</span>uzr --password<span class=o>=</span>XXXX beepify &gt; /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>/beepify.sql

<span class=c1># copy folders to backup folder</span>
cp -r /var/www/html /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>
cp -r /var/www/portfolio /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>
cp -r /var/www/vanhackaton-video-editor /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>

<span class=c1># zip each file or folder to zip\_ folder</span>
<span class=nb>cd</span> /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>/
<span class=k>for</span> dir in <span class=k>$(</span>ls<span class=k>)</span><span class=p>;</span> <span class=k>do</span> tar cvzf /tmp/<span class=nv>$TODAY</span>/<span class=si>${</span><span class=nv>dir</span><span class=si>}</span><span class=se>\_</span><span class=nv>$NOW</span>.tar.gz <span class=si>${</span><span class=nv>dir</span><span class=si>}</span><span class=p>;</span> <span class=k>done</span>

<span class=c1># copy zipped backup files and folders to amazon S3</span>
aws s3 cp /tmp/<span class=nv>$TODAY</span> s3://bucketname/<span class=nv>$TODAY</span> --recursive

<span class=c1># remove the temporary folders</span>
<span class=nb>echo</span> <span class=s2>&#34;removing /tmp/bkp\_</span><span class=nv>$NOW</span><span class=s2> and /tmp/</span><span class=nv>$TODAY</span><span class=s2>&#34;</span>
rm -rf /tmp/bkp<span class=se>\_</span><span class=nv>$NOW</span>
rm -rf /tmp/<span class=nv>$TODAY</span>
</code></pre></div><p>Now let&rsquo;s comment some important parts of this script:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>TODAY=&#34;$(date +&#34;%Y%m%d&#34;)&#34;
</code></pre></div><p>It creates an environment variable with the result of the execution of the command date with the Ymd format, the equivalent is done on NOW.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>mysqldump --complete-insert --no-set-names --host=127.0.0.1 --user=backup\_uzr --password=XXXX wordpress &gt; /tmp/bkp\_$NOW/wordpress.sql
</code></pre></div><p>Note that we are using the password in the command line, so there one security issue here, in order to minimize this I created an read only user at the mysql database, with the following commands, that should be executed at msyql console</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>GRANT SELECT, LOCK TABLES ON \*.\* TO backup\_uzr@127.0.0.1 IDENTIFIED BY &#39;XXXX&#39;; 
GRANT SELECT, LOCK TABLES ON \*.\* TO backup\_uzr@localhost IDENTIFIED BY &#39;XXXX&#39;; 
flush privileges;
</code></pre></div><p>After all the files are copied in different folders at /tmp/bkp_$NOW we run this magic command</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>for dir in $(ls); do tar cvzf /tmp/$TODAY/${dir}\_$NOW.tar.gz ${dir}; done
</code></pre></div><p>That can be split as follow:</p><ul><li>iterate over the result list from the command ls</li><li>for each element on that list set the value to the variable dir</li><li>execute a tar command creating a tar.gz file at the /tmp/$TODAY folder zipping the content of the variable dir</li></ul><p>After that run the aws cli command to push the results to the S3 In order to schedule the script use the command: crontab -e and add the line to run every dy at 04:42 AM</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>42 4 \* \* \* /root/my-nice-backup-script.sh
</code></pre></div><p>Have fun backuping up your sites!</p></div></div><div class=footer><div class=footer-social><span class="social-icon social-icon-twitter"><a href=https://twitter.com/athanazio title=twitter target=_blank rel=noopener><img src=/images/icons/twitter.svg width=24 height=24 alt=twitter></a></span>
<span class="social-icon social-icon-github"><a href=https://github.com/hamilton-lima title=github target=_blank rel=noopener><img src=/images/icons/github.svg width=24 height=24 alt=github></a></span>
<span class="social-icon social-icon-linkedin"><a href=https://www.linkedin.com/in/hamiltonlima title=linkedin target=_blank rel=noopener><img src=/images/icons/linkedin.svg width=24 height=24 alt=linkedin></a></span></div></div></div><script type=text/javascript src=/js/bundle.min.df8701b5f509374aa1fd6b7f39e6100d9eedbb268d023d3bec1a6e8220fd6dee.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-CFNGHJCTX3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-CFNGHJCTX3');</script></body></html>